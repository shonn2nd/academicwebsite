---
title: "SEM (notes for myself)"
authors: 
  - admin
tags: [R, stats]
date: "2025-07-02"
categories: 
  - R
output:
  md_document:
    variant: gfm
    preserve_yaml: true
    toc: false
    toc_depth: 2
math: true
draft: false
---



<div id="p.21" class="section level1">
<h1>p.21</h1>
<div id="covariance-derivation" class="section level2">
<h2>Covariance Derivation</h2>
<p>The covariance between two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is defined as:</p>
<p><span class="math display">\[
\text{Cov}(X_1, X_2) = \mathbb{E}\left[(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))\right]
\]</span></p>
<div id="step-1-expanding-the-expression" class="section level3">
<h3>Step 1: Expanding the expression</h3>
<p>First, expand the product inside the expectation:</p>
<p><span class="math display">\[
(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2)) = X_1X_2 - X_1\mathbb{E}(X_2) - \mathbb{E}(X_1)X_2 + \mathbb{E}(X_1)\mathbb{E}(X_2)
\]</span></p>
</div>
<div id="step-2-taking-the-expectation" class="section level3">
<h3>Step 2: Taking the expectation</h3>
<p>Now, take the expectation of both sides:</p>
<p><span class="math display">\[
\mathbb{E}\left[(X_1 - \mathbb{E}(X_1))(X_2 - \mathbb{E}(X_2))\right] = \mathbb{E}(X_1X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2) + \mathbb{E}(X_1)\mathbb{E}(X_2)
\]</span></p>
</div>
<div id="step-3-simplifying-the-result" class="section level3">
<h3>Step 3: Simplifying the result</h3>
<p>Notice that the terms <span class="math inline">\(\mathbb{E}(X_1)\mathbb{E}(X_2)\)</span> cancel out:</p>
<p><span class="math display">\[
\mathbb{E}(X_1X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2)
\]</span></p>
<p>Thus, we arrive at the final formula for the covariance:</p>
<p><span class="math display">\[
\text{Cov}(X_1, X_2) = \mathbb{E}(X_1X_2) - \mathbb{E}(X_1)\mathbb{E}(X_2)
\]</span></p>
<hr />
</div>
</div>
</div>
<div id="p.22" class="section level1">
<h1>p.22</h1>
<div id="covariance-derivation-in-confirmatory-factor-analysis" class="section level2">
<h2>Covariance Derivation in Confirmatory Factor Analysis</h2>
<p>We start with the observed variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>:</p>
<p><span class="math display">\[
x_1 = \lambda_1 \xi_1 + \delta_1
\]</span>
<span class="math display">\[
x_2 = \lambda_2 \xi_1 + \delta_2
\]</span></p>
<p>We want to find <span class="math inline">\(\text{Cov}(x_1, x_2)\)</span>:</p>
<p><span class="math display">\[
\text{Cov}(x_1, x_2) = \text{Cov}(\lambda_1 \xi_1 + \delta_1, \lambda_2 \xi_1 + \delta_2)
\]</span></p>
<p>Using bilinearity of covariance:</p>
<p><span class="math display">\[
= \lambda_1 \lambda_2 \text{Cov}(\xi_1, \xi_1) + \lambda_1 \text{Cov}(\xi_1, \delta_2) + \lambda_2 \text{Cov}(\delta_1, \xi_1) + \text{Cov}(\delta_1, \delta_2)
\]</span></p>
<p>Assuming:<br />
- <span class="math inline">\(\text{Cov}(\xi_1, \delta_2) = 0\)</span><br />
- <span class="math inline">\(\text{Cov}(\delta_1, \xi_1) = 0\)</span><br />
- <span class="math inline">\(\text{Cov}(\delta_1, \delta_2) = 0\)</span><br />
- <span class="math inline">\(\text{Var}(\xi_1) = \phi_{11}\)</span><br />
</p>
<p>Then:</p>
<p><span class="math display">\[
\text{Cov}(x_1, x_2) = \lambda_1 \lambda_2 \phi_{11}
\]</span></p>
<hr />
</div>
</div>
<div id="p.23" class="section level1">
<h1>p.23</h1>
<div id="derivation-covx-c-0" class="section level2">
<h2>1. Derivation: Cov(x, c′) = 0</h2>
<p>Let <span class="math inline">\(\mathbf{x}\)</span> be a random vector and <span class="math inline">\(\mathbf{c}\)</span> a constant vector.</p>
<p><span class="math display">\[
\text{Cov}(\mathbf{x}, \mathbf{c}^\top) = \mathbb{E} \left[ (\mathbf{x} - \mathbb{E}[\mathbf{x}])(\mathbf{c}^\top - \mathbb{E}[\mathbf{c}^\top]) \right]
\]</span></p>
<p>But since <span class="math inline">\(\mathbf{c}^\top\)</span> is constant, <span class="math inline">\(\mathbb{E}[\mathbf{c}^\top] = \mathbf{c}^\top\)</span>, so:</p>
<p><span class="math display">\[
\mathbf{c}^\top - \mathbb{E}[\mathbf{c}^\top] = \mathbf{0}
\]</span></p>
<p>Hence:</p>
<p><span class="math display">\[
\text{Cov}(\mathbf{x}, \mathbf{c}^\top) = \mathbb{E} \left[ (\mathbf{x} - \mathbb{E}[\mathbf{x}]) \cdot \mathbf{0} \right] = \mathbf{0}
\]</span></p>
</div>
<div id="derivation-varx-covx-x-σ" class="section level2">
<h2>2. Derivation: Var(x) = Cov(x, x′) = Σ</h2>
<p>By definition:</p>
<p><span class="math display">\[
\text{Var}(\mathbf{x}) = \text{Cov}(\mathbf{x}, \mathbf{x}^\top) = \mathbb{E} \left[ (\mathbf{x} - \mathbb{E}[\mathbf{x}])(\mathbf{x} - \mathbb{E}[\mathbf{x}])^\top \right]
\]</span></p>
<p>This is the population covariance matrix, denoted as:</p>
<p><span class="math display">\[
\boldsymbol{\Sigma}
\]</span></p>
</div>
<div id="numerical-example-in-r" class="section level2">
<h2>3. Numerical Example in R</h2>
<pre class="r"><code># Set seed and generate data
set.seed(123)

# Simulate random vector x (5 observations, 3 variables)
x &lt;- matrix(c(1, 2, 3, 4, 5,
              2, 3, 4, 5, 6,
              5, 4, 3, 2, 1), ncol = 3)

x</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    2    5
## [2,]    2    3    4
## [3,]    3    4    3
## [4,]    4    5    2
## [5,]    5    6    1</code></pre>
<pre class="r"><code>x_centered &lt;- scale(x, center = TRUE, scale = FALSE)
x_centered</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]   -2   -2    2
## [2,]   -1   -1    1
## [3,]    0    0    0
## [4,]    1    1   -1
## [5,]    2    2   -2
## attr(,&quot;scaled:center&quot;)
## [1] 3 4 3</code></pre>
</div>
<div id="confirm-varx-covx-x" class="section level2">
<h2>4. Confirm Var(x) = Cov(x, x′)</h2>
<pre class="r"><code># Compute covariance matrix of x manually
# population covariance matrix
Sigma_manual_p &lt;- t(x_centered) %*% x_centered / (nrow(x))
Sigma_manual_p</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    2    2   -2
## [2,]    2    2   -2
## [3,]   -2   -2    2</code></pre>
<pre class="r"><code># sample covariance matrix
Sigma_manual_s &lt;- t(x_centered) %*% x_centered / (nrow(x) - 1)
Sigma_manual_s</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  2.5  2.5 -2.5
## [2,]  2.5  2.5 -2.5
## [3,] -2.5 -2.5  2.5</code></pre>
<pre class="r"><code># Compare with built-in cov()
# sample covariance matrix
Sigma_builtin &lt;- cov(x)
Sigma_builtin</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  2.5  2.5 -2.5
## [2,]  2.5  2.5 -2.5
## [3,] -2.5 -2.5  2.5</code></pre>
<hr />
</div>
</div>
<div id="p.28" class="section level1">
<h1>p.28</h1>
<div id="step-1-create-the-data-for-disposable-income-and-consumers-expenditures" class="section level2">
<h2>Step 1: Create the data for Disposable Income and Consumers’ Expenditures</h2>
<pre class="r"><code>income &lt;- c(433, 483, 479, 486, 494, 498, 511, 534, 478, 440, 372, 381, 419, 449, 511, 520, 477, 517, 548, 629)

consum &lt;- c(394, 423, 437, 434, 447, 447, 466, 474, 439, 399, 350, 364, 392, 416, 463, 469, 444, 471, 494, 529)</code></pre>
<div id="step-2-combine-the-data-into-a-matrix" class="section level3">
<h3>Step 2: Combine the data into a matrix</h3>
<pre class="r"><code>data &lt;- cbind(consum, income)

data</code></pre>
<pre><code>##       consum income
##  [1,]    394    433
##  [2,]    423    483
##  [3,]    437    479
##  [4,]    434    486
##  [5,]    447    494
##  [6,]    447    498
##  [7,]    466    511
##  [8,]    474    534
##  [9,]    439    478
## [10,]    399    440
## [11,]    350    372
## [12,]    364    381
## [13,]    392    419
## [14,]    416    449
## [15,]    463    511
## [16,]    469    520
## [17,]    444    477
## [18,]    471    517
## [19,]    494    548
## [20,]    529    629</code></pre>
</div>
<div id="step-3-center-the-data-by-subtracting-the-mean-of-each-variable" class="section level3">
<h3>Step 3: Center the data by subtracting the mean of each variable</h3>
<pre class="r"><code>Z &lt;- scale(data, center = TRUE, scale = FALSE)  # Centers the data (but doesn&#39;t scale it)
Z</code></pre>
<pre><code>##       consum  income
##  [1,]  -43.6  -49.95
##  [2,]  -14.6    0.05
##  [3,]   -0.6   -3.95
##  [4,]   -3.6    3.05
##  [5,]    9.4   11.05
##  [6,]    9.4   15.05
##  [7,]   28.4   28.05
##  [8,]   36.4   51.05
##  [9,]    1.4   -4.95
## [10,]  -38.6  -42.95
## [11,]  -87.6 -110.95
## [12,]  -73.6 -101.95
## [13,]  -45.6  -63.95
## [14,]  -21.6  -33.95
## [15,]   25.4   28.05
## [16,]   31.4   37.05
## [17,]    6.4   -5.95
## [18,]   33.4   34.05
## [19,]   56.4   65.05
## [20,]   91.4  146.05
## attr(,&quot;scaled:center&quot;)
## consum income 
## 437.60 482.95</code></pre>
</div>
<div id="step-4-compute-z-transpose-of-z" class="section level3">
<h3>Step 4: Compute Z’ (transpose of Z)</h3>
<pre class="r"><code>Z_t &lt;- t(Z)
Z_t</code></pre>
<pre><code>##          [,1]   [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10]   [,11]
## consum -43.60 -14.60 -0.60 -3.60  9.40  9.40 28.40 36.40  1.40 -38.60  -87.60
## income -49.95   0.05 -3.95  3.05 11.05 15.05 28.05 51.05 -4.95 -42.95 -110.95
##          [,12]  [,13]  [,14] [,15] [,16] [,17] [,18] [,19]  [,20]
## consum  -73.60 -45.60 -21.60 25.40 31.40  6.40 33.40 56.40  91.40
## income -101.95 -63.95 -33.95 28.05 37.05 -5.95 34.05 65.05 146.05
## attr(,&quot;scaled:center&quot;)
## consum income 
## 437.60 482.95</code></pre>
</div>
<div id="step-5-compute-zz-matrix-multiplication-of-z-and-z" class="section level3">
<h3>Step 5: Compute Z’Z (matrix multiplication of Z’ and Z)</h3>
<pre class="r"><code>Z_t_Z &lt;- Z_t %*% Z
Z_t_Z</code></pre>
<pre><code>##         consum   income
## consum 35886.8 47584.60
## income 47584.6 64992.95</code></pre>
</div>
<div id="step-6-apply-the-1n-1-factor" class="section level3">
<h3>Step 6: Apply the 1/(N-1) factor</h3>
<pre class="r"><code>N &lt;- nrow(Z)  # Number of observations (rows)
cov_matrix_manual &lt;- (1 / (N - 1)) * Z_t_Z

# Display the result
print(cov_matrix_manual)</code></pre>
<pre><code>##          consum   income
## consum 1888.779 2504.453
## income 2504.453 3420.682</code></pre>
<hr />
</div>
</div>
</div>
<div id="p.28-1" class="section level1">
<h1>p.28</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In multivariate statistics, it is often important to detect observations that deviate significantly from the center of the multivariate data cloud. One useful tool is the matrix:</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{Z(Z&#39;Z)^{-1}Z&#39;}
\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\mathbf{Z}\)</span> is the mean-centered data matrix of size <span class="math inline">\(N \times (p + q)\)</span>,</p>
<p><span class="math inline">\(\mathbf{Z}&#39;\mathbf{Z}\)</span> is the cross-product matrix,</p>
<p><span class="math inline">\((\mathbf{Z}&#39;\mathbf{Z})^{-1}\)</span> is its inverse,</p>
<p><span class="math inline">\(\mathbf{A}\)</span> is a square <span class="math inline">\(N \times N\)</span> matrix whose diagonal entries <span class="math inline">\(a_{ii}\)</span> represent the multivariate “distance” of each observation from the center.</p>
<p>The average value of <span class="math inline">\(a_{ii}\)</span> is:</p>
<p><span class="math display">\[
\frac{(p + q)}{N}
\]</span></p>
<p>Observations with much higher <span class="math inline">\(a_{ii}\)</span> values than the average are potential <strong>multivariate outliers</strong>.</p>
</div>
<div id="step-by-step-example" class="section level2">
<h2>Step-by-Step Example</h2>
<p>We will use a simple example dataset with 3 observations and 2 variables.</p>
<pre class="r"><code># Define the data matrix (3 observations, 2 variables)
X &lt;- matrix(c(2, 3, 4, 4, 6, 5), ncol = 2, byrow = FALSE)
colnames(X) &lt;- c(&quot;X1&quot;, &quot;X2&quot;)
rownames(X) &lt;- paste0(&quot;Obs&quot;, 1:3)
X</code></pre>
<pre><code>##      X1 X2
## Obs1  2  4
## Obs2  3  6
## Obs3  4  5</code></pre>
</div>
<div id="step-1-center-the-data-create-z" class="section level2">
<h2>Step 1: Center the Data (Create Z)</h2>
<p>We subtract the mean from each variable to obtain the matrix <span class="math inline">\(\mathbf{Z}\)</span>.</p>
<pre class="r"><code>Z &lt;- scale(X, center = TRUE, scale = FALSE)
Z</code></pre>
<pre><code>##      X1 X2
## Obs1 -1 -1
## Obs2  0  1
## Obs3  1  0
## attr(,&quot;scaled:center&quot;)
## X1 X2 
##  3  5</code></pre>
</div>
<div id="step-2-compute-mathbfzmathbfz" class="section level2">
<h2>Step 2: Compute <span class="math inline">\(\mathbf{Z}&#39;\mathbf{Z}\)</span></h2>
<pre class="r"><code>ZtZ &lt;- t(Z) %*% Z
ZtZ</code></pre>
<pre><code>##    X1 X2
## X1  2  1
## X2  1  2</code></pre>
</div>
<div id="step-3-compute-mathbfzmathbfz-1" class="section level2">
<h2>Step 3: Compute <span class="math inline">\((\mathbf{Z}&#39;\mathbf{Z})^{-1}\)</span></h2>
<pre class="r"><code>ZtZ_inv &lt;- solve(ZtZ)
ZtZ_inv</code></pre>
<pre><code>##            X1         X2
## X1  0.6666667 -0.3333333
## X2 -0.3333333  0.6666667</code></pre>
</div>
<div id="step-4-compute-mathbfa-mathbfzmathbfzmathbfz-1mathbfz" class="section level2">
<h2>Step 4: Compute <span class="math inline">\(\mathbf{A} = \mathbf{Z}(\mathbf{Z}&#39;\mathbf{Z})^{-1}\mathbf{Z}&#39;\)</span></h2>
<pre class="r"><code>A &lt;- Z %*% ZtZ_inv %*% t(Z)
round(A, 3)</code></pre>
<pre><code>##        Obs1   Obs2   Obs3
## Obs1  0.667 -0.333 -0.333
## Obs2 -0.333  0.667 -0.333
## Obs3 -0.333 -0.333  0.667</code></pre>
</div>
<div id="step-5-extract-diagonal-elements-a_ii" class="section level2">
<h2>Step 5: Extract Diagonal Elements <span class="math inline">\(a_{ii}\)</span></h2>
<p>These diagonal values represent the multivariate distance for each observation.</p>
<pre class="r"><code>a_ii &lt;- diag(A)
names(a_ii) &lt;- rownames(X)
a_ii</code></pre>
<pre><code>##      Obs1      Obs2      Obs3 
## 0.6666667 0.6666667 0.6666667</code></pre>
</div>
<div id="step-6-compare-to-expected-average" class="section level2">
<h2>Step 6: Compare to Expected Average</h2>
<pre class="r"><code>p_plus_q &lt;- ncol(Z)  # total number of observed variables (p + q)
N &lt;- nrow(Z)  # number of observations
expected_mean &lt;- p_plus_q / N
expected_mean</code></pre>
<pre><code>## [1] 0.6666667</code></pre>
<p>Any <span class="math inline">\(a_{ii}\)</span> significantly greater than 0.6666667 may indicate a multivariate outlier.</p>
</div>
<div id="step-7-visualization" class="section level2">
<h2>Step 7: Visualization</h2>
<pre class="r"><code>barplot(a_ii, names.arg = names(a_ii), 
        main = &quot;Multivariate Distance (a_ii) for Each Observation&quot;, 
        ylab = &quot;a_ii&quot;, col = &quot;skyblue&quot;, ylim = c(0, 1))
abline(h = expected_mean, col = &quot;red&quot;, lty = 2)
legend(&quot;topright&quot;, legend = paste(&quot;Expected Mean =&quot;, round(expected_mean, 3)), 
       col = &quot;red&quot;, lty = 2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<ul>
<li>Matrix <span class="math inline">\(\mathbf{A} = \mathbf{Z(Z&#39;Z)^{-1}Z&#39;}\)</span> provides a way to measure the multivariate distance of each observation.</li>
<li>Diagonal values <span class="math inline">\(a_{ii}\)</span> indicate how far each case is from the multivariate mean.</li>
<li>The average of the <span class="math inline">\(a_{ii}\)</span>’s is <span class="math inline">\(\frac{(p + q)}{N}\)</span>, which provides a benchmark.</li>
<li>Observations with high <span class="math inline">\(a_{ii}\)</span> values are flagged as potential <strong>outliers</strong> in multivariate space.</li>
</ul>
<p>This method is especially helpful in the context of SEM, factor analysis, or other multivariate procedures where unusual cases may affect model fit or estimates.</p>
</div>
</div>
